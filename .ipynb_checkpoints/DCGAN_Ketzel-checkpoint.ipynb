{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv \n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.utils.data as datatorch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "#from google.colab import drive\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19\n",
    "import math\n",
    "from torchvision.utils import save_image, make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv(\"cosmology_aux_data_170429/labeled.csv\", dtype = np.int64).to_numpy()\n",
    "scored_data = pd.read_csv(\"cosmology_aux_data_170429/scored.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dir = 'cosmology_aux_data_170429/labeled'\n",
    "query_dir = 'cosmology_aux_data_170429/query'\n",
    "scored_dir = 'cosmology_aux_data_170429/scored'\n",
    "labeled_files = os.listdir(labeled_dir)\n",
    "scored_files = os.listdir(scored_dir)\n",
    "query_files = os.listdir(query_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"bsize\" : 32,# Batch size during training.\n",
    "    'imsize' : 228,# Spatial size of training images. All images will be resized to this size during preprocessing.\n",
    "    'nc' : 1,# Number of channles in the training images. For coloured images this is 3.\n",
    "    'nz' : 100,# Size of the Z latent vector (the input to the generator).\n",
    "    'ngf' : 128,# Size of feature maps in the generator. The depth will be multiples of this.\n",
    "    'ndf' : 128, # Size of features maps in the discriminator. The depth will be multiples of this.\n",
    "    'nepochs' : 2000,# Number of training epochs.\n",
    "    'start_epoch': 0, #Epoch number to start from\n",
    "    'lr' : 0.0002,# Learning rate for optimizers\n",
    "    'beta1' : 0.5,# Beta1 hyperparam for Adam optimizer\n",
    "    'save_epoch' : 100}# Save step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_array,dir, mode = 'train', labels = None, params = None):\n",
    "        self.files = file_array\n",
    "        self.mode = mode\n",
    "        self.labels  = labels\n",
    "        self.dir = dir\n",
    "        self.img_size = params['imsize']\n",
    "        # Transforms for low resolution images and high resolution images\n",
    "        self.lr_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.img_size, self.img_size), Image.BICUBIC),\n",
    "                #transforms.Grayscale( num_output_channels=1) ,\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                transforms.Normalize([0.5], [0.5])\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.dir, self.files[index]))\n",
    "        img_lr = self.lr_transform(img)\n",
    "        #img_hr = self.hr_transform(img)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            mask = np.isin(self.labels[:,0], int(self.files[index][:-4]))\n",
    "            label = self.labels[mask,1]\n",
    "            \n",
    "            return img_lr,label\n",
    "        else:\n",
    "            return img_lr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scored_dataset = ImageDataset(scored_files, scored_dir, mode = 'train', labels =scored_data, params = params)\n",
    "labeled_dataset = ImageDataset(labeled_files, labeled_dir, mode = 'train', labels =labeled_data, params = params)\n",
    "query_dataset = ImageDataset(query_files, query_dir, mode = 'test', labels =None, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input is the latent vector Z.\n",
    "        self.tconv1 = nn.ConvTranspose2d(params['nz'], params['ngf']*8,\n",
    "            kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(params['ngf']*8)\n",
    "\n",
    "        # Input Dimension: (ngf*8) x 4 x 4\n",
    "        self.tconv2 = nn.ConvTranspose2d(params['ngf']*8, params['ngf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ngf']*4)\n",
    "\n",
    "        # Input Dimension: (ngf*4) x 8 x 8\n",
    "        self.tconv3 = nn.ConvTranspose2d(params['ngf']*4, params['ngf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ngf']*2)\n",
    "\n",
    "        # Input Dimension: (ngf*2) x 16 x 16\n",
    "        self.tconv4 = nn.ConvTranspose2d(params['ngf']*2, params['ngf'],\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ngf'])\n",
    "\n",
    "        # Input Dimension: (ngf) * 32 * 32\n",
    "        self.tconv5 = nn.ConvTranspose2d(params['ngf'], params['nc'],\n",
    "            4, 2, 1, bias=False)\n",
    "        #Output Dimension: (nc) x 64 x 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.tconv1(x)))\n",
    "        x = F.relu(self.bn2(self.tconv2(x)))\n",
    "        x = F.relu(self.bn3(self.tconv3(x)))\n",
    "        x = F.relu(self.bn4(self.tconv4(x)))\n",
    "\n",
    "        x = torch.tanh(self.tconv5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define the Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input Dimension: (nc) x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(params['nc'], params['ndf'],\n",
    "            4, 2, 1, bias=False)\n",
    "\n",
    "        # Input Dimension: (ndf) x 32 x 32\n",
    "        self.conv2 = nn.Conv2d(params['ndf'], params['ndf']*2,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(params['ndf']*2)\n",
    "\n",
    "        # Input Dimension: (ndf*2) x 16 x 16\n",
    "        self.conv3 = nn.Conv2d(params['ndf']*2, params['ndf']*4,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(params['ndf']*4)\n",
    "\n",
    "        # Input Dimension: (ndf*4) x 8 x 8\n",
    "        self.conv4 = nn.Conv2d(params['ndf']*4, params['ndf']*8,\n",
    "            4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(params['ndf']*8)\n",
    "\n",
    "        # Input Dimension: (ndf*8) x 4 x 4\n",
    "        self.conv5 = nn.Conv2d(params['ndf']*8, 1, 4, 1, 0, bias=False)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, True)\n",
    "\n",
    "        #x = torch.sigmoid(self.conv5(x))\n",
    "        x = torch.sigmoid(torch.sum(self.flatten1(self.conv5(x)),dim=1))\n",
    "        #x = torch.sigmoid(self.flatten1(self.conv5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"saved_images\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "netG = Generator(params).to(device)\n",
    "netD = Discriminator(params).to(device)\n",
    "\n",
    "# Losses\n",
    "criterion = nn.BCELoss(reduction = 'sum')\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "fixed_noise = torch.randn(1, params['nz'], 1, 1, device=device)\n",
    "print(fixed_noise.size())\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Optimizer for the discriminator.\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "# Optimizer for the generator.\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "\n",
    "# Stores generated images as training progresses.\n",
    "img_list = []\n",
    "# Stores generator losses during training.\n",
    "G_losses = []\n",
    "# Stores discriminator losses during training.\n",
    "D_losses = []\n",
    "\n",
    "iters = 0\n",
    "\n",
    "labeled_Dataloader = datatorch.DataLoader(dataset=labeled_dataset, shuffle=False, batch_size=params['bsize'])\n",
    "scored_Dataloader = datatorch.DataLoader(dataset=scored_dataset, shuffle=False,  batch_size=params['bsize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['start_epoch'] != 0:\n",
    "    # Load pretrained models\n",
    "    netG.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"))\n",
    "    netD.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[0/2000][0/38]\tLoss_D: 844.2170\tLoss_G: 422.4333\tD(x): 0.0000\tD(G(z)): 0.3914 / 0.0000\n",
      "True\n",
      "[1/2000][0/38]\tLoss_D: 828.9336\tLoss_G: 291.8629\tD(x): 0.0000\tD(G(z)): 0.0001 / 0.0001\n",
      "True\n",
      "[2/2000][0/38]\tLoss_D: 828.9348\tLoss_G: 288.1044\tD(x): 0.0000\tD(G(z)): 0.0001 / 0.0001\n",
      "True\n",
      "[3/2000][0/38]\tLoss_D: 828.9335\tLoss_G: 296.7819\tD(x): 0.0000\tD(G(z)): 0.0001 / 0.0001\n",
      "True\n",
      "[4/2000][0/38]\tLoss_D: 828.9327\tLoss_G: 305.7546\tD(x): 0.0000\tD(G(z)): 0.0001 / 0.0000\n",
      "True\n",
      "[5/2000][0/38]\tLoss_D: 828.9319\tLoss_G: 314.3237\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[6/2000][0/38]\tLoss_D: 828.9315\tLoss_G: 322.3776\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[7/2000][0/38]\tLoss_D: 828.9316\tLoss_G: 321.0355\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[8/2000][0/38]\tLoss_D: 828.9316\tLoss_G: 323.4621\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[9/2000][0/38]\tLoss_D: 828.9314\tLoss_G: 329.2324\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[10/2000][0/38]\tLoss_D: 828.9316\tLoss_G: 326.6669\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[11/2000][0/38]\tLoss_D: 828.9314\tLoss_G: 330.0439\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[12/2000][0/38]\tLoss_D: 828.9312\tLoss_G: 334.5062\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[13/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 343.7514\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[14/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 346.4987\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[15/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 359.4736\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[16/2000][0/38]\tLoss_D: 828.9309\tLoss_G: 358.4845\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[17/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 345.4311\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[18/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 351.1983\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[19/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 348.6326\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[20/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 350.0290\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[21/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 351.3293\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[22/2000][0/38]\tLoss_D: 828.9310\tLoss_G: 348.4241\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[23/2000][0/38]\tLoss_D: 828.9309\tLoss_G: 356.3550\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[24/2000][0/38]\tLoss_D: 828.9309\tLoss_G: 358.6460\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[25/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 364.1973\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[26/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 369.4352\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[27/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 371.9642\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[28/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 374.4037\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[29/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 367.0063\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[30/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 369.3025\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[31/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 371.1974\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[32/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 375.3948\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[33/2000][0/38]\tLoss_D: 828.9308\tLoss_G: 375.4069\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[34/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 389.3354\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[35/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 395.4273\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[36/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 397.1584\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[37/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 408.6994\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[38/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 413.4089\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[39/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 414.0848\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[40/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 388.4591\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[41/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 390.2605\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[42/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 399.6583\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[43/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 403.4897\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[44/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 406.2183\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[45/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 409.3619\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[46/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 412.6805\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[47/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 418.8317\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "True\n",
      "[48/2000][0/38]\tLoss_D: 828.9307\tLoss_G: 418.2326\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "broken data stream when reading image file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c0bdaa37bf81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nepochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlr_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_Dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Transfer data tensor to GPU/CPU (device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#label = label.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ad89d8c2c1f0>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mimg_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m#img_hr = self.hr_transform(img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box)\u001b[0m\n\u001b[0;32m   1888\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1890\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mLOAD_TRUNCATED_IMAGES\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[1;31m# still raised if decoder fails to return anything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m             \u001b[0mraise_ioerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mraise_ioerror\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"decoder error %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" when reading image file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: broken data stream when reading image file"
     ]
    }
   ],
   "source": [
    "netG.train()\n",
    "netD.train()\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, (lr_img, label) in enumerate(labeled_Dataloader, 0):\n",
    "        # Transfer data tensor to GPU/CPU (device)\n",
    "        #label = label.to(device)\n",
    "        lr_img = lr_img[label==1,:,:].unsqueeze(1).to(device)\n",
    "        \n",
    "        # Get batch size. Can be different from params['nbsize'] for last batch in epoch.\n",
    "        b_size = lr_img.size(0)\n",
    "        label = torch.full((b_size, ), real_label, device=device)\n",
    "        \n",
    "        # Make accumalated gradients of the discriminator zero.\n",
    "        netD.zero_grad()\n",
    "        # Create labels for the real data. (label=1)\n",
    "        label = torch.full((b_size, ), real_label, device=device)\n",
    "        \n",
    "        output = netD(lr_img)\n",
    "        \n",
    "        errD_real = criterion(output, label)\n",
    "        \n",
    "        # Calculate gradients for backpropagation.\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        # Sample random data from a unit normal distribution.\n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "        # Generate fake data (images).\n",
    "        fake_data = netG(noise)\n",
    "        # Create labels for fake data. (label=0)\n",
    "        label.fill_(fake_label)\n",
    "        # Calculate the output of the discriminator of the fake data.\n",
    "        # As no gradients w.r.t. the generator parameters are to be\n",
    "        # calculated, detach() is used. Hence, only gradients w.r.t. the\n",
    "        # discriminator parameters will be calculated.\n",
    "        # This is done because the loss functions for the discriminator\n",
    "        # and the generator are slightly different.\n",
    "        \n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate gradients for backpropagation.\n",
    "        \n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        \n",
    "        # Net discriminator loss.\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update discriminator parameters.\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Make accumalted gradients of the generator zero.\n",
    "        netG.zero_grad()\n",
    "        # We want the fake data to be classified as real. Hence\n",
    "        # real_label are used. (label=1)\n",
    "        label.fill_(real_label)\n",
    "        # No detach() is used here as we want to calculate the gradients w.r.t.\n",
    "        # the generator this time.\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        \n",
    "        # Gradients for backpropagation are calculated.\n",
    "        # Gradients w.r.t. both the generator and the discriminator\n",
    "        # parameters are calculated, however, the generator's optimizer\n",
    "        # will only update the parameters of the generator. The discriminator\n",
    "        # gradients will be set to zero in the next iteration by netD.zero_grad()\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update generator parameters.\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Check progress of training.\n",
    "        if i%50 == 0:\n",
    "            print(torch.cuda.is_available())\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, params['nepochs'], i, len(labeled_Dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save the losses for plotting.\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        # Check how the generator is doing by saving G's output on a fixed noise.\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(labeled_Dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = netG(fixed_noise).detach().cpu()\n",
    "                gen_hr = make_grid(fake_data, nrow=1, normalize=True)\n",
    "                save_image(gen_hr, \"saved_images/%d.png\" % iters, normalize=False)\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    # Save the model.\n",
    "    if epoch % params['save_epoch'] == 1:\n",
    "        torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'saved_models/model_epoch_{}.pth'.format(epoch))\n",
    "        \n",
    "    \n",
    "# Save the final trained model.\n",
    "torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'saved_models/model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
